import streamlit as st
import boto3
import json
from datetime import datetime

# Page configuration
st.set_page_config(
    page_title="Driver Risk Assistant",
    page_icon="üöó",
    layout="wide"
)

model_id_list = [
    "amazon.nova-pro-v1:0",
    "google.gemma-3-27b-it",
    "qwen.qwen3-next-80b-a3b",
    "global.anthropic.claude-haiku-4-5-20251001-v1:0",
    "global.anthropic.claude-sonnet-4-5-20250929-v1:0",
]

# Initialize Bedrock client
@st.cache_resource
def get_bedrock_client():
    return boto3.client('bedrock-runtime', region_name='us-east-1')

bedrock_runtime = get_bedrock_client()

# Title and description
st.title("üöó Driver Risk Assistant")
st.markdown("**Real-time telemetry analysis using Amazon Bedrock - Multiple Approaches**")

# Create tabs
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "üîÑ Streaming (Prompt)", 
    "üìã Tool Use (Schema Enforced)",
    "üìù Prompt Engineering (AWS Blog)",
    "üõ†Ô∏è Tool Use (AWS Blog)",
    "üìö Guide"
])

# Sidebar for configuration
with st.sidebar:
    st.header("‚öôÔ∏è Configuration")
    model_id = st.selectbox(
        "Model",
        model_id_list,
        index=3,
        help="Select model"
    )
    temperature = st.slider("Temperature", 0.0, 1.0, 0.0, 0.1)
    max_tokens = st.slider("Max Tokens", 100, 2000, 1000, 50)

    st.divider()
    st.markdown("### About")
    st.info("""
    **4 Approaches:**

    1. Streaming (real-time)
    2. Tool Use with Converse
    3. Prompt Engineering (AWS)
    4. Tool Use with invoke_model (AWS)
    """)

    st.divider()
    st.markdown("### 2025 AWS Guidance")
    st.success("""
    **Success Rates:**
    - Prompt: 93%+
    - Tool Use: 95%+ ‚úÖ

    Tool Use recommended for production!
    """)

# Shared telemetry inputs function
def render_telemetry_inputs(tab_key):
    col1, col2 = st.columns(2)

    with col1:
        speed = st.number_input("Speed (mph)", min_value=0, max_value=150, value=85, key=f"speed_{tab_key}")
        speed_limit = st.number_input("Speed Limit (mph)", min_value=0, max_value=100, value=65, key=f"speed_limit_{tab_key}")
        hard_braking = st.number_input("Hard Braking Events", min_value=0, max_value=20, value=3, key=f"hard_braking_{tab_key}")
        rapid_acceleration = st.number_input("Rapid Acceleration Events", min_value=0, max_value=20, value=2, key=f"rapid_accel_{tab_key}")

    with col2:
        sharp_turns = st.number_input("Sharp Turns", min_value=0, max_value=20, value=1, key=f"sharp_turns_{tab_key}")
        driver_rating = st.selectbox("Driver Rating", [1, 2, 3, 4, 5], index=1, key=f"driver_rating_{tab_key}")
        trip_duration = st.number_input("Trip Duration (minutes)", min_value=1, max_value=300, value=45, key=f"trip_duration_{tab_key}")
        time_of_day = st.selectbox("Time of Day", ["Morning", "Afternoon", "Evening", "Night"], key=f"time_of_day_{tab_key}")

    return {
        "speed_mph": speed,
        "speed_limit_mph": speed_limit,
        "speed_over_limit": speed - speed_limit,
        "hard_braking_events": hard_braking,
        "rapid_acceleration_events": rapid_acceleration,
        "sharp_turns": sharp_turns,
        "driver_rating": driver_rating,
        "trip_duration_minutes": trip_duration,
        "time_of_day": time_of_day
    }

# Display results function
def display_results(risk_assessment, latency_info, token_info, mode="streaming"):
    # Success message
    if mode == "streaming":
        st.success(f"‚úÖ Analysis complete - Total: {latency_info['total']:.0f}ms | First Token: {latency_info['first_token']:.0f}ms")
    else:
        st.success(f"‚úÖ Analysis complete in {latency_info['total']:.0f}ms")

    # Create columns for key metrics
    if mode == "streaming":
        metric_col1, metric_col2, metric_col3, metric_col4 = st.columns(4)
    else:
        metric_col1, metric_col2, metric_col3 = st.columns(3)

    with metric_col1:
        risk_level = risk_assessment.get('risk_level', 'unknown').upper()
        color = {"LOW": "üü¢", "MEDIUM": "üü°", "HIGH": "üî¥"}.get(risk_level, "‚ö™")
        st.metric("Risk Level", f"{color} {risk_level}")

    with metric_col2:
        risk_score = risk_assessment.get('risk_score', 0)
        st.metric("Risk Score", f"{risk_score}/100")

    with metric_col3:
        st.metric("Total Time", f"{latency_info['total']:.0f}ms")

    if mode == "streaming":
        with metric_col4:
            st.metric("First Token", f"{latency_info['first_token']:.0f}ms")

    # Display insight
    st.subheader("üí° Risk Insight")
    st.info(risk_assessment.get('insight', 'No insight provided'))

    # Display factors and recommendations in columns
    factor_col, rec_col = st.columns(2)

    with factor_col:
        st.subheader("‚ö†Ô∏è Risk Factors")
        factors = risk_assessment.get('factors', [])
        if factors:
            for factor in factors:
                st.markdown(f"- {factor}")
        else:
            st.write("No specific factors identified")

    with rec_col:
        st.subheader("‚úÖ Recommendations")
        recommendations = risk_assessment.get('recommendations', [])
        if recommendations:
            for rec in recommendations:
                st.markdown(f"- {rec}")
        else:
            st.write("No recommendations provided")

    # Display full JSON response
    with st.expander("üîç View Full JSON Response"):
        st.json(risk_assessment)

    # Display API metadata
    with st.expander("üìä API Response Metadata"):
        metadata = {
            "model_id": model_id,
            "mode": mode,
            "total_latency_ms": round(latency_info['total'], 2),
            "input_tokens": token_info['input'],
            "output_tokens": token_info['output'],
            "total_tokens": token_info['input'] + token_info['output'],
        }

        if mode == "streaming":
            metadata["first_token_latency_ms"] = round(latency_info['first_token'], 2)

        st.json(metadata)

        # Calculate approximate cost
        if "claude-3-haiku" in model_id or "claude-haiku" in model_id:
            input_cost = token_info['input'] * 0.00025 / 1000
            output_cost = token_info['output'] * 0.00125 / 1000
        elif "claude-3-5-sonnet" in model_id or "claude-sonnet" in model_id:
            input_cost = token_info['input'] * 0.003 / 1000
            output_cost = token_info['output'] * 0.015 / 1000
        else:
            input_cost = token_info['input'] * 0.0003 / 1000
            output_cost = token_info['output'] * 0.0006 / 1000

        total_cost = input_cost + output_cost
        st.metric("Estimated Cost", f"${total_cost:.6f}")

# TAB 1: Streaming Mode
with tab1:
    st.info("‚ö° **Real-time streaming** with prompt-based JSON generation")

    st.subheader("üìä Telemetry Input")
    telemetry_data_stream = render_telemetry_inputs("streaming")

    st.subheader("üìù Prompt Template")
    prompt_template_stream = st.text_area(
        "System Prompt",
        value="""You are a driver risk assessment system. Analyze the telemetry data and provide a structured risk assessment in JSON format.

The output must include:
- risk_level: "low", "medium", or "high"
- risk_score: numeric value from 0-100
- insight: brief natural language explanation (1-2 sentences)
- factors: array of contributing risk factors
- recommendations: array of safety recommendations

Telemetry Data:
{telemetry}

Provide your assessment in valid JSON format only, with no additional text.""",
        height=300,
        key="prompt_stream"
    )

    if st.button("üîç Generate (Streaming)", type="primary", use_container_width=True, key="btn_stream"):
        formatted_prompt = prompt_template_stream.format(
            telemetry=json.dumps(telemetry_data_stream, indent=2)
        )

        with st.expander("üìã View Formatted Telemetry Data"):
            st.json(telemetry_data_stream)

        st.subheader("üîÑ Streaming Response")
        stream_placeholder = st.empty()

        start_time = datetime.now()
        accumulated_text = ""
        first_token_time = None
        input_tokens = 0
        output_tokens = 0

        try:
            response = bedrock_runtime.converse_stream(
                modelId=model_id,
                messages=[{"role": "user", "content": [{"text": formatted_prompt}]}],
                inferenceConfig={"temperature": temperature, "maxTokens": max_tokens}
            )

            for event in response['stream']:
                if 'contentBlockDelta' in event:
                    delta = event['contentBlockDelta']['delta']
                    if 'text' in delta:
                        chunk = delta['text']
                        accumulated_text += chunk
                        if first_token_time is None:
                            first_token_time = datetime.now()
                        stream_placeholder.code(accumulated_text, language="json")
                elif 'metadata' in event:
                    usage = event['metadata'].get('usage', {})
                    input_tokens = usage.get('inputTokens', 0)
                    output_tokens = usage.get('outputTokens', 0)

            end_time = datetime.now()
            total_latency = (end_time - start_time).total_seconds() * 1000
            first_token_latency = (first_token_time - start_time).total_seconds() * 1000 if first_token_time else 0

            output_text = accumulated_text
            if "```json" in output_text:
                output_text = output_text.split("```json")[1].split("```")[0].strip()
            elif "```" in output_text:
                output_text = output_text.split("```")[1].split("```")[0].strip()

            risk_assessment = json.loads(output_text)

            display_results(
                risk_assessment,
                {"total": total_latency, "first_token": first_token_latency},
                {"input": input_tokens, "output": output_tokens},
                mode="streaming"
            )

        except json.JSONDecodeError as e:
            st.error(f"‚ùå JSON Parsing Error: {str(e)}")
            st.code(accumulated_text, language="text")
        except Exception as e:
            st.error(f"‚ùå Error: {str(e)}")
            st.exception(e)

# TAB 2: Structured Output with Tool Use (Converse)
with tab2:
    st.success("‚úÖ **Schema-enforced** output using Tool Use with Converse API")

    st.subheader("üìä Telemetry Input")
    telemetry_data_struct = render_telemetry_inputs("structured")

    st.subheader("üìê JSON Schema Definition")

    tool_schema = {
        "name": "generate_risk_assessment",
        "description": "Generate a structured driver risk assessment based on telemetry data",
        "inputSchema": {
            "json": {
                "type": "object",
                "properties": {
                    "risk_level": {
                        "type": "string",
                        "enum": ["low", "medium", "high"],
                        "description": "Overall risk classification"
                    },
                    "risk_score": {
                        "type": "number",
                        "minimum": 0,
                        "maximum": 100,
                        "description": "Numeric risk score from 0-100"
                    },
                    "insight": {
                        "type": "string",
                        "description": "Brief natural language explanation of the risk assessment"
                    },
                    "factors": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "List of contributing risk factors"
                    },
                    "recommendations": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "List of safety recommendations"
                    }
                },
                "required": ["risk_level", "risk_score", "insight", "factors", "recommendations"]
            }
        }
    }

    with st.expander("üîç View Tool Schema", expanded=True):
        st.json(tool_schema)

    schema_editable = st.checkbox("Edit Schema", value=False, key="edit_schema_tab2")
    if schema_editable:
        schema_text = st.text_area(
            "Edit JSON Schema",
            value=json.dumps(tool_schema, indent=2),
            height=400,
            key="schema_edit"
        )
        try:
            tool_schema = json.loads(schema_text)
        except json.JSONDecodeError:
            st.error("Invalid JSON schema")

    st.subheader("üìù User Prompt")
    user_prompt_struct = st.text_area(
        "Prompt",
        value="""Analyze the following driver telemetry data and provide a risk assessment:

{telemetry}

Consider factors like speeding, aggressive driving behaviors, time of day, and driver rating.""",
        height=200,
        key="prompt_struct"
    )

    if st.button("üîç Generate (Tool Use - Converse)", type="primary", use_container_width=True, key="btn_struct"):
        formatted_prompt = user_prompt_struct.format(
            telemetry=json.dumps(telemetry_data_struct, indent=2)
        )

        with st.expander("üìã View Formatted Telemetry Data"):
            st.json(telemetry_data_struct)

        with st.spinner("Analyzing with schema enforcement..."):
            start_time = datetime.now()

            try:
                response = bedrock_runtime.converse(
                    modelId=model_id,
                    messages=[{"role": "user", "content": [{"text": formatted_prompt}]}],
                    inferenceConfig={"temperature": temperature, "maxTokens": max_tokens},
                    toolConfig={
                        "tools": [{"toolSpec": tool_schema}],
                        "toolChoice": {"tool": {"name": "generate_risk_assessment"}}
                    }
                )

                end_time = datetime.now()
                total_latency = (end_time - start_time).total_seconds() * 1000

                content = response['output']['message']['content']
                tool_use_block = next((block['toolUse'] for block in content if 'toolUse' in block), None)

                if tool_use_block:
                    risk_assessment = tool_use_block['input']
                    input_tokens = response['usage']['inputTokens']
                    output_tokens = response['usage']['outputTokens']

                    st.info("üéØ **Schema Enforcement**: Output guaranteed to match the defined JSON schema!")

                    display_results(
                        risk_assessment,
                        {"total": total_latency, "first_token": 0},
                        {"input": input_tokens, "output": output_tokens},
                        mode="structured"
                    )

                    with st.expander("üîß View Raw Tool Use Response"):
                        st.json({
                            "toolUseId": tool_use_block.get('toolUseId'),
                            "name": tool_use_block.get('name'),
                            "input": tool_use_block.get('input')
                        })
                else:
                    st.error("No tool use found in response")
                    st.json(response)

            except Exception as e:
                st.error(f"‚ùå Error: {str(e)}")
                st.exception(e)

# TAB 3: Prompt Engineering (AWS Blog Example)
with tab3:
    st.warning("üìù **AWS Blog Approach**: Prompt Engineering (June 2025) - 93%+ success rate")

    st.markdown("""
    **From AWS Blog**: "Structured data response with Amazon Bedrock"

    This approach uses careful prompt engineering to guide the model to produce structured JSON.
    """)

    st.subheader("üìä Telemetry Input")
    telemetry_data_prompt = render_telemetry_inputs("prompt_eng")

    st.subheader("Step 1: Define JSON Schema")

    json_schema = {
        "type": "object",
        "properties": {
            "risk_level": {
                "type": "string",
                "enum": ["low", "medium", "high"],
                "description": "Overall risk classification"
            },
            "risk_score": {
                "type": "number",
                "minimum": 0,
                "maximum": 100,
                "description": "Numeric risk score"
            },
            "insight": {
                "type": "string",
                "maxLength": 200,
                "description": "Brief summary"
            },
            "factors": {
                "type": "array",
                "items": {"type": "string"},
                "description": "Risk factors"
            },
            "recommendations": {
                "type": "array",
                "items": {"type": "string"},
                "description": "Safety recommendations"
            }
        },
        "required": ["risk_level", "risk_score", "insight", "factors", "recommendations"]
    }

    with st.expander("üîç View JSON Schema"):
        st.json(json_schema)

    st.subheader("Step 2: Craft the Prompt")

    prompt_aws = st.text_area(
        "Prompt (AWS Blog Style)",
        value="""You are an AI assistant that analyzes driver telemetry and returns structured JSON data.

Your task:
1. Read the telemetry data provided in the <input> tags
2. Analyze the risk level and contributing factors
3. Return a JSON response that strictly follows this schema:

{schema}

Example output:
{{
    "risk_level": "high",
    "risk_score": 85,
    "insight": "Excessive speeding and aggressive braking indicate high-risk driving behavior",
    "factors": ["Speeding 20+ mph over limit", "Multiple hard braking events"],
    "recommendations": ["Reduce speed", "Maintain safe following distance"]
}}

Rules:
- risk_level must be "low", "medium", or "high"
- risk_score must be 0-100
- insight must be under 200 characters
- Always include all required fields
- Return ONLY valid JSON, no additional text

<input>
{telemetry}
</input>""",
        height=500,
        key="prompt_aws"
    )

    if st.button("üîç Generate (Prompt Engineering)", type="primary", use_container_width=True, key="btn_prompt_aws"):
        with st.spinner("Generating with prompt engineering..."):
            try:
                start_time = datetime.now()

                formatted_prompt = prompt_aws.format(
                    schema=json.dumps(json_schema, indent=2),
                    telemetry=json.dumps(telemetry_data_prompt, indent=2)
                )

                # Use invoke_model as shown in AWS blog
                if "anthropic.claude" in model_id or "claude" in model_id:
                    body = json.dumps({
                        "anthropic_version": "bedrock-2023-05-31",
                        "max_tokens": max_tokens,
                        "temperature": temperature,
                        "messages": [
                            {
                                "role": "user",
                                "content": [{"type": "text", "text": formatted_prompt}]
                            }
                        ]
                    })

                    response = bedrock_runtime.invoke_model(
                        modelId=model_id,
                        body=body
                    )

                    response_body = json.loads(response['body'].read())
                    output_text = response_body['content'][0]['text']
                    input_tokens = response_body['usage']['input_tokens']
                    output_tokens = response_body['usage']['output_tokens']
                else:
                    # Fallback to converse for non-Claude models
                    response = bedrock_runtime.converse(
                        modelId=model_id,
                        messages=[{"role": "user", "content": [{"text": formatted_prompt}]}],
                        inferenceConfig={"temperature": temperature, "maxTokens": max_tokens}
                    )
                    output_text = response['output']['message']['content'][0]['text']
                    input_tokens = response['usage']['inputTokens']
                    output_tokens = response['usage']['outputTokens']

                end_time = datetime.now()
                total_latency = (end_time - start_time).total_seconds() * 1000

                # Parse JSON
                if "```json" in output_text:
                    output_text = output_text.split("```json")[1].split("```")[0].strip()
                elif "```" in output_text:
                    output_text = output_text.split("```")[1].split("```")[0].strip()

                risk_assessment = json.loads(output_text)

                st.info("üìù **Prompt Engineering**: Output guided by careful prompt crafting (93%+ success rate)")

                display_results(
                    risk_assessment,
                    {"total": total_latency, "first_token": 0},
                    {"input": input_tokens, "output": output_tokens},
                    mode="prompt_engineering"
                )

            except json.JSONDecodeError as e:
                st.error(f"‚ùå JSON Parsing Error: {str(e)}")
                st.code(output_text, language="text")
            except Exception as e:
                st.error(f"‚ùå Error: {str(e)}")
                st.exception(e)

# TAB 4: Tool Use with invoke_model (AWS Blog Example)
with tab4:
    st.success("üõ†Ô∏è **AWS Blog Approach**: Tool Use with invoke_model (June 2025) - 95%+ success rate")

    st.markdown("""
    **From AWS Blog**: "Structured data response with Amazon Bedrock"

    This approach uses Tool Use with the `invoke_model` API for schema-enforced output.

    **Note**: `invoke_model` has model-specific formats. For Claude models, we use Anthropic's native format.
    """)

    st.subheader("üìä Telemetry Input")
    telemetry_data_tool_aws = render_telemetry_inputs("tool_aws")

    st.subheader("Step 1: Define Tool with Schema")

    # Tool definition for Claude's native format (used with invoke_model)
    tool_definition_claude = {
        "name": "analyze_driver_risk",
        "description": "Analyze driver telemetry and return structured risk assessment",
        "input_schema": {
            "type": "object",
            "properties": {
                "risk_level": {
                    "type": "string",
                    "enum": ["low", "medium", "high"],
                    "description": "Overall risk classification"
                },
                "risk_score": {
                    "type": "number",
                    "minimum": 0,
                    "maximum": 100,
                    "description": "Numeric risk score"
                },
                "insight": {
                    "type": "string",
                    "description": "Brief explanation"
                },
                "factors": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "Risk factors"
                },
                "recommendations": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "Safety recommendations"
                }
            },
            "required": ["risk_level", "risk_score", "insight", "factors", "recommendations"]
        }
    }

    with st.expander("üîç View Tool Definition (Claude Native Format)", expanded=True):
        st.json(tool_definition_claude)
        st.info("""
        **Note**: When using `invoke_model` with Claude, the tool format is:
        - `input_schema` (not `inputSchema`)
        - Direct properties (not wrapped in `json`)
        - This is Claude's native format
        """)

    st.subheader("Step 2: Create Message")

    message_aws = st.text_area(
        "User Message",
        value="""Analyze the following driver telemetry data:

<input>
{telemetry}
</input>

Use the analyze_driver_risk tool to provide a structured risk assessment.""",
        height=150,
        key="message_aws"
    )

    if st.button("üîç Generate (Tool Use - invoke_model)", type="primary", use_container_width=True, key="btn_tool_aws"):
        with st.spinner("Generating with Tool Use (invoke_model)..."):
            try:
                start_time = datetime.now()

                formatted_message = message_aws.format(
                    telemetry=json.dumps(telemetry_data_tool_aws, indent=2)
                )

                # Check if model is Claude
                if "anthropic.claude" in model_id or "claude" in model_id:
                    # Use invoke_model with Claude's native tool format
                    body = json.dumps({
                        "anthropic_version": "bedrock-2023-05-31",
                        "max_tokens": max_tokens,
                        "temperature": temperature,
                        "messages": [
                            {
                                "role": "user",
                                "content": formatted_message
                            }
                        ],
                        "tools": [tool_definition_claude],
                        "tool_choice": {
                            "type": "tool",
                            "name": "analyze_driver_risk"
                        }
                    })

                    st.code(f"""
# Request body for invoke_model (Claude native format):
{{
    "anthropic_version": "bedrock-2023-05-31",
    "messages": [...],
    "tools": [
        {{
            "name": "analyze_driver_risk",
            "input_schema": {{...}}  # Note: input_schema, not inputSchema
        }}
    ],
    "tool_choice": {{
        "type": "tool",
        "name": "analyze_driver_risk"
    }}
}}
                    """, language="python")

                    response = bedrock_runtime.invoke_model(
                        modelId=model_id,
                        body=body
                    )

                    response_body = json.loads(response['body'].read())

                    # Extract tool use from response
                    tool_use_content = None
                    for content_block in response_body.get('content', []):
                        if content_block.get('type') == 'tool_use':
                            tool_use_content = content_block
                            break

                    if tool_use_content:
                        risk_assessment = tool_use_content['input']
                        input_tokens = response_body['usage']['input_tokens']
                        output_tokens = response_body['usage']['output_tokens']

                        end_time = datetime.now()
                        total_latency = (end_time - start_time).total_seconds() * 1000

                        st.success("‚úÖ Tool use successful with invoke_model!")

                        with st.expander("üîç View Raw Response"):
                            st.json(response_body)

                        st.info("üõ†Ô∏è **Tool Use (invoke_model)**: Schema-enforced output using Claude's native format (95%+ success rate)")

                        display_results(
                            risk_assessment,
                            {"total": total_latency, "first_token": 0},
                            {"input": input_tokens, "output": output_tokens},
                            mode="tool_use_aws"
                        )
                    else:
                        st.error("No tool use found in response")
                        st.json(response_body)

                else:
                    # For non-Claude models, use converse API instead
                    st.warning("‚ö†Ô∏è invoke_model with tools is Claude-specific. Using converse API instead for this model.")

                    # Convert to converse format
                    tool_spec_converse = {
                        "name": "analyze_driver_risk",
                        "description": "Analyze driver telemetry and return structured risk assessment",
                        "inputSchema": {
                            "json": tool_definition_claude["input_schema"]
                        }
                    }

                    response = bedrock_runtime.converse(
                        modelId=model_id,
                        messages=[{"role": "user", "content": [{"text": formatted_message}]}],
                        inferenceConfig={"temperature": temperature, "maxTokens": max_tokens},
                        toolConfig={
                            "tools": [{"toolSpec": tool_spec_converse}],
                            "toolChoice": {"tool": {"name": "analyze_driver_risk"}}
                        }
                    )

                    content = response['output']['message']['content']
                    tool_use_block = next((block['toolUse'] for block in content if 'toolUse' in block), None)

                    if tool_use_block:
                        risk_assessment = tool_use_block['input']
                        input_tokens = response['usage']['inputTokens']
                        output_tokens = response['usage']['outputTokens']

                        end_time = datetime.now()
                        total_latency = (end_time - start_time).total_seconds() * 1000

                        st.info("üõ†Ô∏è **Tool Use (converse fallback)**: Schema-enforced output (95%+ success rate)")

                        display_results(
                            risk_assessment,
                            {"total": total_latency, "first_token": 0},
                            {"input": input_tokens, "output": output_tokens},
                            mode="tool_use_aws"
                        )
                    else:
                        st.error("No tool use found in response")

            except Exception as e:
                st.error(f"‚ùå Error: {str(e)}")
                st.exception(e)

                st.markdown("""
                ### Troubleshooting:

                **Common Issues with invoke_model + tools:**

                1. **Format Differences**: 
                   - `invoke_model` uses model-native formats
                   - Claude uses `input_schema` (not `inputSchema`)
                   - Other models may not support tools with `invoke_model`

                2. **Recommendation**: 
                   - Use `converse` API (Tab 2) for cross-model compatibility
                   - Use `invoke_model` only when you need model-specific features

                3. **AWS Blog Context**:
                   - The blog examples use Claude-specific formats
                   - For production, `converse` API is more portable
                """)

# TAB 5: Complete Guide & Comparison
with tab5:
    st.header("üìö Complete Guide: Structured Output with Amazon Bedrock")
    st.markdown("*Based on AWS Blog (June 2025) and Best Practices*")

    # Overview
    st.divider()
    st.subheader("üéØ Overview: Why Structured Output Matters")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("""
        ### The Challenge

        LLMs are trained primarily on **unstructured text**:
        - Articles, books, websites
        - Natural language conversations
        - Few examples of structured formats

        **Result**: Models struggle with:
        - ‚ùå Consistent JSON formatting
        - ‚ùå Adhering to specific schemas
        - ‚ùå Type validation
        - ‚ùå Required field enforcement
        """)

    with col2:
        st.markdown("""
        ### The Solution

        Amazon Bedrock offers **multiple approaches**:
        - üìù Prompt Engineering (flexible)
        - üõ†Ô∏è Tool Use (schema-enforced)
        - üîÑ Streaming (real-time UX)
        - üîÄ Hybrid approaches

        **Benefits**:
        - ‚úÖ Reliable structured data
        - ‚úÖ API/database integration
        - ‚úÖ Regulatory compliance
        - ‚úÖ Automated workflows
        """)

    # Detailed Comparison
    st.divider()
    st.subheader("üìä Detailed Approach Comparison")

    # Create expandable sections for each approach
    with st.expander("üîÑ **Tab 1: Streaming (Prompt-based)** - Real-time UX", expanded=False):
        col1, col2 = st.columns([2, 1])

        with col1:
            st.markdown("""
            ### How It Works

            1. **API**: `converse_stream()`
            2. **Method**: Prompt engineering guides JSON output
            3. **Streaming**: Tokens arrive in real-time
            4. **Validation**: Manual post-processing required

            ### Code Pattern
            ```python
            response = bedrock_runtime.converse_stream(
                modelId=model_id,
                messages=[{
                    "role": "user",
                    "content": [{"text": prompt_with_json_instructions}]
                }]
            )

            for event in response['stream']:
                if 'contentBlockDelta' in event:
                    chunk = event['contentBlockDelta']['delta']['text']
                    # Display chunk in real-time
            ```

            ### Key Characteristics
            - ‚ö° **First token latency**: 100-200ms
            - üìä **Success rate**: ~93%
            - üîß **Schema enforcement**: None (prompt-based)
            - üé® **Flexibility**: High (any format)
            """)

        with col2:
            st.success("**Best For:**")
            st.markdown("""
            - Chat interfaces
            - Real-time feedback
            - User-facing apps
            - Progressive display
            """)

            st.warning("**Limitations:**")
            st.markdown("""
            - No schema guarantee
            - Manual validation
            - Parsing required
            - ~7% failure rate
            """)

    with st.expander("üìã **Tab 2: Tool Use (Converse API)** - Production Ready ‚≠ê", expanded=True):
        col1, col2 = st.columns([2, 1])

        with col1:
            st.markdown("""
            ### How It Works

            1. **API**: `converse()`
            2. **Method**: Tool Use with JSON Schema
            3. **Validation**: Runtime schema enforcement by Bedrock
            4. **Output**: Pre-validated structured data

            ### Code Pattern
            ```python
            tool_schema = {
                "name": "generate_output",
                "inputSchema": {
                    "json": {
                        "type": "object",
                        "properties": {...},
                        "required": [...]
                    }
                }
            }

            response = bedrock_runtime.converse(
                modelId=model_id,
                messages=[...],
                toolConfig={
                    "tools": [{"toolSpec": tool_schema}],
                    "toolChoice": {"tool": {"name": "generate_output"}}
                }
            )

            # Extract validated output
            output = response['output']['message']['content'][0]['toolUse']['input']
            ```

            ### Key Characteristics
            - ‚ö° **Latency**: 200-400ms
            - üìä **Success rate**: ~95%
            - üîß **Schema enforcement**: ‚úÖ Automatic
            - üé® **Flexibility**: JSON only
            - üõ°Ô∏è **Validation**: Built-in
            """)

        with col2:
            st.success("**Best For:** ‚≠ê")
            st.markdown("""
            - **Production apps**
            - **Regulatory compliance**
            - **Complex schemas**
            - **Database integration**
            - **API responses**
            - **Insurance/finance**
            """)

            st.info("**Advantages:**")
            st.markdown("""
            - ‚úÖ Schema guaranteed
            - ‚úÖ Type validation
            - ‚úÖ Required fields
            - ‚úÖ Enum constraints
            - ‚úÖ No parsing needed
            """)

    with st.expander("üìù **Tab 3: Prompt Engineering (invoke_model)** - Flexible", expanded=False):
        col1, col2 = st.columns([2, 1])

        with col1:
            st.markdown("""
            ### How It Works

            1. **API**: `invoke_model()`
            2. **Method**: Detailed prompt with schema definition
            3. **Format**: Model-specific request/response
            4. **Validation**: Manual post-processing

            ### Code Pattern
            ```python
            # Claude-specific format
            body = json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "messages": [{
                    "role": "user",
                    "content": prompt_with_schema_and_examples
                }],
                "max_tokens": 1000,
                "temperature": 0.0
            })

            response = bedrock_runtime.invoke_model(
                modelId="anthropic.claude-...",
                body=body
            )

            # Parse response (model-specific)
            result = json.loads(response['body'].read())
            output = result['content'][0]['text']
            ```

            ### Key Characteristics
            - ‚ö° **Latency**: 200-500ms
            - üìä **Success rate**: 93%+ (AWS testing)
            - üîß **Schema enforcement**: None
            - üé® **Flexibility**: Very high (XML, CSV, JSON)
            - üîÑ **Model-specific**: Yes
            """)

        with col2:
            st.success("**Best For:**")
            st.markdown("""
            - Rapid prototyping
            - Multiple formats
            - Simple schemas
            - Experimentation
            - Non-JSON output
            """)

            st.warning("**Considerations:**")
            st.markdown("""
            - Model-specific code
            - Manual validation
            - Prompt fragility
            - Parsing complexity
            """)

    with st.expander("üõ†Ô∏è **Tab 4: Tool Use (invoke_model)** - AWS Blog Pattern", expanded=False):
        col1, col2 = st.columns([2, 1])

        with col1:
            st.markdown("""
            ### How It Works

            1. **API**: `invoke_model()` with tools
            2. **Method**: Model-native tool format
            3. **Format**: Claude uses `input_schema` (not `inputSchema`)
            4. **Validation**: Runtime schema enforcement

            ### Code Pattern
            ```python
            # Claude-native tool format
            tool = {
                "name": "generate_output",
                "input_schema": {  # Note: input_schema, not inputSchema
                    "type": "object",
                    "properties": {...},
                    "required": [...]
                }
            }

            body = json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "messages": [...],
                "tools": [tool],
                "tool_choice": {"type": "tool", "name": "generate_output"}
            })

            response = bedrock_runtime.invoke_model(
                modelId="anthropic.claude-...",
                body=body
            )

            # Extract tool use
            result = json.loads(response['body'].read())
            tool_use = next(c for c in result['content'] if c['type'] == 'tool_use')
            output = tool_use['input']
            ```

            ### Key Characteristics
            - ‚ö° **Latency**: 200-400ms
            - üìä **Success rate**: 95%+ (AWS testing)
            - üîß **Schema enforcement**: ‚úÖ Automatic
            - üé® **Flexibility**: JSON only
            - üîÑ **Model-specific**: Yes (Claude format shown)
            """)

        with col2:
            st.success("**Best For:**")
            st.markdown("""
            - AWS blog patterns
            - Claude-specific apps
            - Model-native features
            - Advanced use cases
            """)

            st.info("**Note:**")
            st.markdown("""
            - Claude-specific format
            - Use Tab 2 for portability
            - `converse` is recommended
            """)

    # Critical Differences
    st.divider()
    st.subheader("üîë Critical Differences Explained")

    diff_tab1, diff_tab2, diff_tab3 = st.tabs([
        "API Differences",
        "Schema Enforcement",
        "Format Differences"
    ])

    with diff_tab1:
        st.markdown("""
        ### API Comparison: converse vs invoke_model

        | Aspect | `converse` / `converse_stream` | `invoke_model` |
        |--------|-------------------------------|----------------|
        | **Interface** | Unified across models | Model-specific |
        | **Request Format** | Standardized | Native to each model |
        | **Response Format** | Standardized | Native to each model |
        | **Tool Format** | `toolSpec` with `inputSchema.json` | Model-native (e.g., `input_schema` for Claude) |
        | **Streaming** | ‚úÖ `converse_stream()` | ‚ùå Not available |
        | **Multi-turn** | ‚úÖ Built-in | Manual implementation |
        | **Portability** | ‚úÖ High (same code, different models) | ‚ùå Low (rewrite per model) |
        | **AWS Recommendation** | ‚úÖ **Recommended** | Use for model-specific features |

        ### Example: Same Task, Different APIs
        """)

        col1, col2 = st.columns(2)

        with col1:
            st.code("""
# converse API (portable)
response = bedrock_runtime.converse(
    modelId=any_model_id,  # Works with any model
    messages=[{
        "role": "user",
        "content": [{"text": "..."}]
    }],
    inferenceConfig={
        "temperature": 0.0,
        "maxTokens": 1000
    }
)

# Extract response (same for all models)
output = response['output']['message']['content'][0]['text']
            """, language="python")
            st.success("‚úÖ **Portable**: Same code works across models")

        with col2:
            st.code("""
# invoke_model API (Claude-specific)
body = json.dumps({
    "anthropic_version": "bedrock-2023-05-31",
    "messages": [{
        "role": "user",
        "content": "..."
    }],
    "max_tokens": 1000,
    "temperature": 0.0
})

response = bedrock_runtime.invoke_model(
    modelId="anthropic.claude-...",
    body=body
)

# Extract response (Claude-specific)
result = json.loads(response['body'].read())
output = result['content'][0]['text']
            """, language="python")
            st.warning("‚ö†Ô∏è **Model-specific**: Different code per model")

    with diff_tab2:
        st.markdown("""
        ### Schema Enforcement: How It Works

        #### Without Tool Use (Prompt Engineering)
        """)

        st.code("""
# You write a prompt
prompt = '''
Return JSON with these fields:
- name (string)
- age (number, 0-120)
- status (enum: "active", "inactive")
'''

# Model generates text
response = "Here's the data: {name: 'John', age: 25, status: 'active'}"

# Problems:
# ‚ùå Wrong format (missing quotes)
# ‚ùå No validation (age could be 999)
# ‚ùå Typos possible (status: "activ")
# ‚ùå Missing fields possible
# ‚ùå Extra fields possible

# You must manually:
# 1. Parse the text
# 2. Validate types
# 3. Check ranges
# 4. Verify required fields
# 5. Handle errors
        """, language="python")

        st.markdown("#### With Tool Use (Schema Enforced)")

        st.code("""
# You define a schema
tool_schema = {
    "name": "generate_data",
    "inputSchema": {
        "json": {
            "type": "object",
            "properties": {
                "name": {"type": "string"},
                "age": {"type": "number", "minimum": 0, "maximum": 120},
                "status": {"type": "string", "enum": ["active", "inactive"]}
            },
            "required": ["name", "age", "status"]
        }
    }
}

# Bedrock enforces the schema
response = bedrock_runtime.converse(
    messages=[...],
    toolConfig={"tools": [{"toolSpec": tool_schema}]}
)

# Output is GUARANTEED to:
# ‚úÖ Be valid JSON
# ‚úÖ Have correct types (age is number)
# ‚úÖ Meet constraints (age 0-120)
# ‚úÖ Match enum (status is "active" or "inactive")
# ‚úÖ Include all required fields
# ‚úÖ No extra fields

# You get validated data directly:
output = response['output']['message']['content'][0]['toolUse']['input']
# {"name": "John", "age": 25, "status": "active"}
        """, language="python")

        st.success("""
        ### Key Insight

        **Prompt Engineering**: You ask nicely, hope for the best, validate manually

        **Tool Use**: Bedrock enforces the contract, guarantees compliance
        """)

    with diff_tab3:
        st.markdown("""
        ### Format Differences: converse vs invoke_model

        #### Tool Definition Format
        """)

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("**converse API** (Unified)")
            st.code("""
{
    "toolSpec": {
        "name": "my_tool",
        "description": "...",
        "inputSchema": {
            "json": {
                "type": "object",
                "properties": {...}
            }
        }
    }
}

# Same format for all models!
            """, language="json")

        with col2:
            st.markdown("**invoke_model** (Claude Native)")
            st.code("""
{
    "name": "my_tool",
    "description": "...",
    "input_schema": {
        "type": "object",
        "properties": {...}
    }
}

# Different per model!
# Note: input_schema, not inputSchema
# Note: No "json" wrapper
            """, language="json")

        st.warning("""
        ### Important Differences

        | Feature | converse | invoke_model (Claude) |
        |---------|----------|----------------------|
        | Wrapper | `toolSpec` | None |
        | Schema key | `inputSchema` | `input_schema` |
        | Schema wrapper | `json: {...}` | Direct `{...}` |
        | Tool choice | `toolChoice: {tool: {name: "..."}}` | `tool_choice: {type: "tool", name: "..."}` |

        **Recommendation**: Use `converse` for consistency across models!
        """)

    # Decision Matrix
    st.divider()
    st.subheader("üéØ Decision Matrix: Which Approach to Use?")

    decision_col1, decision_col2 = st.columns(2)

    with decision_col1:
        st.markdown("""
        ### ‚úÖ Use Tool Use (Tabs 2 & 4) When:

        | Requirement | Why Tool Use |
        |-------------|--------------|
        | **Production application** | 95% success rate, reliable |
        | **Regulatory compliance** | Schema guaranteed, auditable |
        | **Database integration** | Validated data, no parsing errors |
        | **API responses** | Consistent structure |
        | **Complex schemas** | Nested objects, arrays, constraints |
        | **Type safety critical** | Runtime validation |
        | **Insurance/finance** | Zero tolerance for errors |
        | **Automated workflows** | Predictable structure |

        **Recommended**: Tab 2 (converse) for portability
        """)

    with decision_col2:
        st.markdown("""
        ### ‚úÖ Use Prompt Engineering (Tabs 1 & 3) When:

        | Requirement | Why Prompt Engineering |
        |-------------|----------------------|
        | **Rapid prototyping** | Faster to implement |
        | **Flexible formats** | XML, CSV, custom formats |
        | **Simple schemas** | 2-3 fields, basic types |
        | **Experimentation** | Quick iterations |
        | **Model doesn't support tools** | No alternative |
        | **Non-JSON output** | Markdown, code, etc. |
        | **Learning/demos** | Easier to understand |
        | **Cost-sensitive** | Slightly cheaper |

        **Recommended**: Tab 1 (streaming) for better UX
        """)

    # Real-world Examples
    st.divider()
    st.subheader("üåç Real-World Use Cases")

    use_case_tabs = st.tabs([
        "Insurance/Risk",
        "Customer Service",
        "Data Extraction",
        "API Integration"
    ])

    with use_case_tabs[0]:
        st.markdown("""
        ### Insurance & Risk Assessment (This App!)

        **Requirement**: Analyze driver telemetry, generate risk scores for insurance calculations

        **Why Tool Use?**
        - ‚úÖ Regulatory reporting requires exact schema
        - ‚úÖ Risk scores must be numeric (0-100)
        - ‚úÖ Risk level must be enum (low/medium/high)
        - ‚úÖ All fields required for actuarial models
        - ‚úÖ No tolerance for parsing errors

        **Schema Example**:
        ```json
        {
            "risk_level": "high",           // Must be enum
            "risk_score": 85,                // Must be 0-100
            "factors": ["speeding", "..."], // Must be array
            "recommendations": [...]         // Required field
        }
        ```

        **Result**: 95%+ success rate, zero schema violations, regulatory compliant
        """)

    with use_case_tabs[1]:
        st.markdown("""
        ### Customer Service Email Analysis

        **Requirement**: Categorize and route customer emails

        **Approach**: Tool Use for structured routing data

        **Schema**:
        ```json
        {
            "category": "complaint",        // Enum: complaint/question/feedback
            "priority": "high",             // Enum: low/medium/high
            "department": "billing",        // Enum: billing/tech/sales
            "sentiment": -0.7,              // Number: -1 to 1
            "requires_response": true       // Boolean
        }
        ```

        **Why Tool Use?**
        - ‚úÖ Automated routing requires exact categories
        - ‚úÖ Priority must be valid enum for SLA tracking
        - ‚úÖ Sentiment must be numeric for analytics
        - ‚úÖ Boolean flags for workflow automation

        **Alternative**: Prompt engineering for email summaries (flexible text)
        """)

    with use_case_tabs[2]:
        st.markdown("""
        ### Document Data Extraction

        **Requirement**: Extract structured data from invoices, contracts, forms

        **Approach**: Tool Use for guaranteed field extraction

        **Schema Example (Invoice)**:
        ```json
        {
            "invoice_number": "INV-2025-001",
            "date": "2025-01-15",
            "total": 1250.00,              // Must be number
            "currency": "USD",             // Must be enum
            "line_items": [                // Must be array
                {
                    "description": "...",
                    "quantity": 5,         // Must be number
                    "unit_price": 250.00   // Must be number
                }
            ],
            "payment_terms": "Net 30"
        }
        ```

        **Why Tool Use?**
        - ‚úÖ Financial data must be exact (numbers, not strings)
        - ‚úÖ Dates must be validated format
        - ‚úÖ Nested structures (line items) need schema
        - ‚úÖ Database insertion requires type safety
        """)

    with use_case_tabs[3]:
        st.markdown("""
        ### API Response Generation

        **Requirement**: Generate API responses for mobile/web apps

        **Approach**: Tool Use for API contract compliance

        **Schema Example (User Profile API)**:
        ```json
        {
            "user_id": "usr_123",
            "profile": {
                "name": "John Doe",
                "email": "john@example.com",
                "verified": true,          // Must be boolean
                "created_at": "2025-01-15T10:30:00Z"
            },
            "preferences": {
                "notifications": true,     // Must be boolean
                "theme": "dark"            // Must be enum
            },
            "stats": {
                "login_count": 42,         // Must be number
                "last_active": "2025-01-20T15:45:00Z"
            }
        }
        ```

        **Why Tool Use?**
        - ‚úÖ API contracts require exact schema
        - ‚úÖ Mobile apps expect specific types
        - ‚úÖ Breaking changes cause app crashes
        - ‚úÖ OpenAPI/Swagger spec compliance
        - ‚úÖ Automated testing requires consistency
        """)

    # Performance Comparison
    st.divider()
    st.subheader("‚ö° Performance Comparison")

    perf_data = {
        "Metric": [
            "First Token Latency",
            "Total Latency",
            "Success Rate",
            "Schema Compliance",
            "Parsing Required",
            "Validation Required",
            "Error Handling",
            "Cost per Request"
        ],
        "Streaming (Tab 1)": [
            "100-200ms ‚ö°",
            "300-500ms",
            "~93%",
            "‚ùå No",
            "‚úÖ Yes",
            "‚úÖ Yes",
            "Manual",
            "$0.0003-0.0006"
        ],
        "Tool Use (Tab 2)": [
            "N/A",
            "200-400ms ‚ö°",
            "~95% ‚úÖ",
            "‚úÖ Yes ‚úÖ",
            "‚ùå No ‚úÖ",
            "‚ùå No ‚úÖ",
            "Built-in ‚úÖ",
            "$0.0003-0.0006"
        ],
        "Prompt Eng (Tab 3)": [
            "N/A",
            "200-500ms",
            "93%+",
            "‚ùå No",
            "‚úÖ Yes",
            "‚úÖ Yes",
            "Manual",
            "$0.0003-0.0006"
        ],
        "Tool Use invoke (Tab 4)": [
            "N/A",
            "200-400ms",
            "95%+ ‚úÖ",
            "‚úÖ Yes ‚úÖ",
            "‚ùå No ‚úÖ",
            "‚ùå No ‚úÖ",
            "Built-in ‚úÖ",
            "$0.0003-0.0006"
        ]
    }

    st.table(perf_data)

    st.info("""
    ### Key Takeaways:

    1. **Latency**: All approaches are similar (200-500ms)
    2. **Success Rate**: Tool Use wins (95% vs 93%)
    3. **Developer Experience**: Tool Use eliminates parsing/validation
    4. **Cost**: Essentially the same across all approaches
    5. **Streaming**: Best for UX, but no schema enforcement
    """)

    # Best Practices
    st.divider()
    st.subheader("üí° Best Practices & Recommendations")

    best_practice_tabs = st.tabs([
        "Production Checklist",
        "Common Pitfalls",
        "Migration Guide"
    ])

    with best_practice_tabs[0]:
        st.markdown("""
        ### ‚úÖ Production Deployment Checklist

        #### For Tool Use (Recommended)

        - [ ] **Schema Design**
          - Define all required fields
          - Use enums for categorical data
          - Set min/max for numeric fields
          - Document each property

        - [ ] **Error Handling**
          - Handle model timeouts
          - Retry logic for transient failures
          - Fallback for unsupported models
          - Log schema violations (shouldn't happen, but log anyway)

        - [ ] **Testing**
          - Test with edge cases (empty arrays, null values)
          - Validate against 1000+ samples
          - Test with different models
          - Load testing for scale

        - [ ] **Monitoring**
          - Track success rates
          - Monitor latency (p50, p95, p99)
          - Alert on schema violations
          - Cost tracking per request

        - [ ] **Documentation**
          - Document schema versions
          - API contract documentation
          - Example requests/responses
          - Migration guides for schema changes

        #### For Prompt Engineering

        - [ ] **Prompt Design**
          - Include schema in prompt
          - Provide 1-2 examples
          - Clear formatting instructions
          - Error handling instructions

        - [ ] **Validation**
          - JSON parsing with error handling
          - Schema validation library (e.g., jsonschema)
          - Type checking
          - Required field verification

        - [ ] **Fallbacks**
          - Retry with clarified prompt
          - Default values for missing fields
          - Human review queue for failures
        """)

    with best_practice_tabs[1]:
        st.markdown("""
        ### ‚ö†Ô∏è Common Pitfalls to Avoid

        #### 1. Wrong API for the Job

        ```python
        # ‚ùå DON'T: Use invoke_model when you need portability
        response = bedrock_runtime.invoke_model(
            modelId="anthropic.claude-...",  # Locked to Claude
            body=claude_specific_format
        )

        # ‚úÖ DO: Use converse for portability
        response = bedrock_runtime.converse(
            modelId=any_model,  # Works with any model
            messages=[...]
        )
        ```

        #### 2. Forgetting Schema Validation (Prompt Engineering)

        ```python
        # ‚ùå DON'T: Trust the output blindly
        output = json.loads(response_text)
        save_to_database(output)  # Could fail!

        # ‚úÖ DO: Validate before using
        output = json.loads(response_text)
        validate_schema(output, expected_schema)
        save_to_database(output)
        ```

        #### 3. Wrong Tool Format

        ```python
        # ‚ùå DON'T: Mix formats
        tool = {
            "toolSpec": {  # converse format
                "input_schema": {...}  # invoke_model format
            }
        }

        # ‚úÖ DO: Use correct format for API
        # For converse:
        tool = {"toolSpec": {"inputSchema": {"json": {...}}}}

        # For invoke_model (Claude):
        tool = {"name": "...", "input_schema": {...}}
        ```

        #### 4. Not Handling Streaming Properly

        ```python
        # ‚ùå DON'T: Assume complete JSON in each chunk
        for event in stream:
            chunk = event['text']
            data = json.loads(chunk)  # Will fail!

        # ‚úÖ DO: Accumulate then parse
        accumulated = ""
        for event in stream:
            accumulated += event['text']
        data = json.loads(accumulated)
        ```

        #### 5. Ignoring Model Capabilities

        ```python
        # ‚ùå DON'T: Use Tool Use with models that don't support it
        response = bedrock_runtime.converse(
            modelId="some-model-without-tool-support",
            toolConfig={...}  # Will fail!
        )

        # ‚úÖ DO: Check model capabilities first
        if model_supports_tools(model_id):
            use_tool_use()
        else:
            use_prompt_engineering()
        ```
        """)

    with best_practice_tabs[2]:
        st.markdown("""
        ### üîÑ Migration Guide

        #### From Prompt Engineering ‚Üí Tool Use

        **Step 1: Extract Your Schema**

        ```python
        # Before (in prompt):
        prompt = '''
        Return JSON with:
        - risk_level: "low", "medium", or "high"
        - risk_score: number 0-100
        - factors: array of strings
        '''

        # After (as tool schema):
        tool_schema = {
            "name": "assess_risk",
            "inputSchema": {
                "json": {
                    "type": "object",
                    "properties": {
                        "risk_level": {
                            "type": "string",
                            "enum": ["low", "medium", "high"]
                        },
                        "risk_score": {
                            "type": "number",
                            "minimum": 0,
                            "maximum": 100
                        },
                        "factors": {
                            "type": "array",
                            "items": {"type": "string"}
                        }
                    },
                    "required": ["risk_level", "risk_score", "factors"]
                }
            }
        }
        ```

        **Step 2: Simplify Your Prompt**

        ```python
        # Before (detailed instructions):
        prompt = '''
        You are a risk assessor. Analyze the data and return JSON.
        The JSON must have these exact fields: ...
        Example: {...}
        Rules: ...
        '''

        # After (simple task description):
        prompt = "Analyze this driver telemetry and assess the risk."
        # Schema enforcement handles the rest!
        ```

        **Step 3: Remove Validation Code**

        ```python
        # Before:
        response = bedrock_runtime.converse(...)
        text = response['output']['message']['content'][0]['text']

        # Parse and validate
        try:
            data = json.loads(text)
            validate_types(data)
            validate_required_fields(data)
            validate_enums(data)
            validate_ranges(data)
        except Exception as e:
            handle_error(e)

        # After:
        response = bedrock_runtime.converse(..., toolConfig={...})
        data = response['output']['message']['content'][0]['toolUse']['input']
        # Already validated! Just use it.
        ```

        **Step 4: Update Error Handling**

        ```python
        # Before: Handle parsing errors
        try:
            data = json.loads(text)
        except json.JSONDecodeError:
            retry_with_better_prompt()

        # After: Handle API errors only
        try:
            response = bedrock_runtime.converse(...)
        except ClientError as e:
            handle_api_error(e)
        # No parsing errors possible!
        ```

        #### From invoke_model ‚Üí converse

        **Step 1: Update Tool Format**

        ```python
        # Before (invoke_model - Claude):
        tool = {
            "name": "my_tool",
            "input_schema": {  # Note: underscore
                "type": "object",
                "properties": {...}
            }
        }

        # After (converse - universal):
        tool = {
            "toolSpec": {
                "name": "my_tool",
                "inputSchema": {  # Note: camelCase
                    "json": {  # Note: wrapped in "json"
                        "type": "object",
                        "properties": {...}
                    }
                }
            }
        }
        ```

        **Step 2: Update API Call**

        ```python
        # Before:
        body = json.dumps({
            "anthropic_version": "bedrock-2023-05-31",
            "messages": [...],
            "tools": [tool],
            "tool_choice": {"type": "tool", "name": "my_tool"}
        })
        response = bedrock_runtime.invoke_model(modelId=..., body=body)
        result = json.loads(response['body'].read())

        # After:
        response = bedrock_runtime.converse(
            modelId=...,
            messages=[...],
            toolConfig={
                "tools": [tool],
                "toolChoice": {"tool": {"name": "my_tool"}}
            }
        )
        # No JSON parsing needed!
        ```

        **Step 3: Update Response Extraction**

        ```python
        # Before:
        content = result['content']
        tool_use = next(c for c in content if c['type'] == 'tool_use')
        data = tool_use['input']

        # After:
        content = response['output']['message']['content']
        tool_use = next(c for c in content if 'toolUse' in c)
        data = tool_use['toolUse']['input']
        ```
        """)

    # Final Recommendations
    st.divider()
    st.subheader("üéØ Final Recommendations")

    rec_col1, rec_col2, rec_col3 = st.columns(3)

    with rec_col1:
        st.success("""
        ### üèÜ For Production

        **Use Tab 2: Tool Use (Converse)**

        ‚úÖ Best success rate (95%+)
        ‚úÖ Schema guaranteed
        ‚úÖ Portable across models
        ‚úÖ No validation code needed
        ‚úÖ Regulatory compliant

        **Perfect for:**
        - Insurance/finance
        - Healthcare
        - Legal/compliance
        - Database integration
        - API responses
        """)

    with rec_col2:
        st.info("""
        ### üöÄ For Prototyping

        **Use Tab 1: Streaming**

        ‚úÖ Fast to implement
        ‚úÖ Real-time feedback
        ‚úÖ Good UX
        ‚úÖ Flexible formats

        **Perfect for:**
        - Demos
        - MVPs
        - User-facing chat
        - Experimentation
        - Learning
        """)

    with rec_col3:
        st.warning("""
        ### üîß For Special Cases

        **Use Tab 3/4: invoke_model**

        ‚ö†Ô∏è Only when needed
        ‚ö†Ô∏è Model-specific features
        ‚ö†Ô∏è Advanced use cases

        **Consider when:**
        - Need model-native features
        - Following AWS blog exactly
        - Performance optimization
        - Legacy code migration
        """)

    # Resources
    st.divider()
    st.subheader("üìö Additional Resources")

    st.markdown("""
    ### Official AWS Documentation

    - [Amazon Bedrock Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html)
    - [Tool Use / Function Calling](https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use.html)
    - [Supported Models and Features](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html)
    - [Prompt Engineering Guidelines](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-engineering-guidelines.html)

    ### AWS Blog Posts

    - **"Structured data response with Amazon Bedrock"** (June 2025) - Source for Tabs 3 & 4
    - **"Generating JSON with the Amazon Bedrock Converse API"** (June 2024) - Original Tool Use guide

    ### Model-Specific Documentation

    - [Anthropic Claude Best Practices](https://docs.anthropic.com/claude/docs/tool-use)
    - [JSON Schema Reference](https://json-schema.org/understanding-json-schema/)

    ### Testing & Validation

    - AWS Testing Results: 93%+ (Prompt), 95%+ (Tool Use)
    - Tested with 1,000 iterations √ó 100 items
    - Complex nested schemas with arrays and diverse types
    """)

    # Summary
    st.divider()
    st.success("""
    ## üéì Summary: Key Takeaways

    1. **Tool Use (Tab 2) is the gold standard** for production applications requiring structured output
    2. **Streaming (Tab 1) provides the best UX** but requires manual validation
    3. **Prompt Engineering (Tab 3) is great for prototyping** and flexible formats
    4. **invoke_model (Tab 4) is model-specific** - use converse for portability
    5. **Schema enforcement eliminates 95%+ of errors** compared to prompt-based approaches
    6. **AWS recommends converse API** for new applications
    7. **Success rates: Tool Use (95%+) > Prompt Engineering (93%+)**
    8. **Cost is similar across all approaches** - choose based on reliability needs

    ### For Your Driver Risk Assistant:

    ‚úÖ **Use Tab 2 (Tool Use with Converse)** because:
    - Insurance/regulatory requirements demand exact schemas
    - Risk scores must be validated (0-100 range)
    - Risk levels must be enums (low/medium/high)
    - No tolerance for parsing errors in production
    - 95%+ success rate meets enterprise standards
    """)

# Footer
st.divider()
st.markdown("""
<div style='text-align: center; color: gray;'>
    <p>Built with Amazon Bedrock | Complete Guide Based on AWS 2025 Best Practices</p>
</div>
""", unsafe_allow_html=True)